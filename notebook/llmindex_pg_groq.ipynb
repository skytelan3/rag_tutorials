{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba06953-0b0b-45f0-ad09-dd43c69f3172",
   "metadata": {},
   "source": [
    "# Building RAG from Scratch\n",
    "https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d104cb-7052-4b17-b6db-2233778fce3b",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e9d01a9-d4f5-410d-a3e8-5190410587f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_TRANSFORMER = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "LLM = 'llama3-70b-8192'\n",
    "LLM_API_KEY = 'your-key'\n",
    "DOCUMENT = '../data/llama2.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e326e-81a6-405d-b81f-7d6473deb9e5",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff261081-aa98-49fa-b9e2-c5ee1559d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/telan3/miniconda3/envs/rag_tutorials/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=SENTENCE_TRANSFORMER)\n",
    "llm = Groq(model=LLM, api_key=LLM_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25213e-2cd8-40b7-af81-1a49494a1cc5",
   "metadata": {},
   "source": [
    "## Initialize Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fab5e19-9201-4ad3-99c7-96930a6e3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"192.168.112.12\"\n",
    "password = \"watchall\"\n",
    "port = \"5432\"\n",
    "user = \"watchtek\"\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82370c94-c430-4ae0-805c-c1a9acbcf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d92cf0-9635-4bdb-abe6-0279f7c4867e",
   "metadata": {},
   "source": [
    "## Build an Ingestion Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafef64f-f852-464d-a157-f29693c5e521",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32bdaacd-35ef-44ab-b736-67789a00e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=DOCUMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b13282-f7ca-4d43-ae15-cbcac9de513f",
   "metadata": {},
   "source": [
    "### 2. Use a Text Splitter to Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b525025-ab04-40d1-b7b6-52407ff10730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57950cec-028b-40e8-8361-c9ff960295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aab675-a2f9-49c0-a2e3-b88e84f4d7ba",
   "metadata": {},
   "source": [
    "### 3. Manually Construct Nodes from Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77f35ab5-cdea-45ba-839b-2f17167b198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638498c1-d73c-4336-8188-7e570e46d042",
   "metadata": {},
   "source": [
    "### 4. Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "355a1c07-b9a4-4df3-90e5-618dec3db403",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b54c-cd57-4d6f-8f3b-0882582c3254",
   "metadata": {},
   "source": [
    "### 5. Load Nodes into a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b39e2034-491a-4277-b9e0-3ff27c779dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0de85b78-ce62-4bb0-87cb-80130422faa4',\n",
       " 'b479f4e4-f2c7-4129-83b9-8d77f75a75f3',\n",
       " '46efff11-da25-4dae-96db-10422e582e24',\n",
       " 'aef71ffa-ba5f-4505-92a2-6242b72b2807',\n",
       " 'dc0cf73c-3b4a-4acc-8bfe-739cd8534f58',\n",
       " 'a1b631b0-a270-497f-af48-379a10aee84a',\n",
       " 'b5ab3f9f-ff87-4bd7-8b07-1437fbce0321',\n",
       " '4fe7c645-a07a-4bd7-8080-f076c450b864',\n",
       " '19133a25-9652-4d46-9cda-8c43090056f1',\n",
       " '92304442-bc40-4ba2-9bcb-5464bed4f06c',\n",
       " '27affe01-0d27-4e25-8f2e-1f2d82df1218',\n",
       " 'e216f022-5084-4c75-b282-fb02cbc64fba',\n",
       " '173140b8-be32-4a85-9c0c-0a18a9edae16',\n",
       " 'fa860ded-dd88-4d10-ad9c-299173a70514',\n",
       " '26285045-d088-4778-8375-d84f97da2677',\n",
       " 'cad6fbfd-ed92-4ec5-aee2-b67306502e7b',\n",
       " '929b5bba-121a-4d0c-b585-623725e1e306',\n",
       " '45f0523e-b5b2-47a7-ab69-0d71ed8e0274',\n",
       " '84653add-7234-40cd-a470-e6faf351e370',\n",
       " '8cc1f510-7c50-49f9-b99b-9627129d7d04',\n",
       " '2449b4c5-e16d-48b8-a4f0-ba1744e43373',\n",
       " '47197e59-9066-40ce-b04c-1d93daf25b31',\n",
       " '819ec27d-fdb8-4c17-b00d-9957c077090f',\n",
       " '051e8d15-8f99-4b95-a926-79f598827190',\n",
       " '1cbfbbbe-0f10-4805-a137-e87ee902976c',\n",
       " '99a2fba1-fcbd-475b-bf9b-6ec74b3ca6b0',\n",
       " 'f61f99a9-9abe-4507-946f-ec97b481c5ce',\n",
       " '08c523dc-b0d8-4c37-8a1a-26475c54a6c2',\n",
       " 'c61aa8a3-3be6-425d-909b-bc0462a13f3d',\n",
       " 'd4ee5e7e-c8f9-4512-85ff-0167b9b783b1',\n",
       " '98f26cf0-f5f0-4529-b62a-4c8bbedf2f26',\n",
       " 'cc49f9f7-7318-4af4-a7fb-135159ef2091',\n",
       " '61f0ff52-9273-4164-a461-ef46de470bc6',\n",
       " '25bbb3ab-c087-41d4-aae6-dce5c6f6c949',\n",
       " '46951aaf-6cd3-4440-ad7e-8b004c66c1ce',\n",
       " 'c1acd5bf-dcd2-478c-95b6-8e26506fa873',\n",
       " '3f5cd38f-467c-45df-9a61-fc9aff343014',\n",
       " '6913ca9a-acca-47e8-90f4-7609cafe7901',\n",
       " 'e46cbc4b-657e-4136-ad26-283dd117729b',\n",
       " '2f6de3ea-0423-4ac2-b31c-6066c3b07f6b',\n",
       " '2952da39-2bf6-407b-ae69-48bfe28d8da5',\n",
       " 'e97bf408-1f16-4e67-a7ff-2732ff3683bd',\n",
       " 'd87ddd02-7bca-4cce-a27f-16c2a0f0815b',\n",
       " '3e686065-ad1e-4b55-b4e2-432106f634f6',\n",
       " 'b34d5a3a-75c1-4385-abe3-849279723e9e',\n",
       " '43ebc708-e96e-4640-bac6-f1b94362aa0c',\n",
       " 'ab13ff06-29e6-4c47-b5f5-76b7ba76eda3',\n",
       " '9b879db5-c606-4d01-95c8-8c7f0402ab26',\n",
       " '49d0570d-0083-41aa-a633-5c99e3f50c39',\n",
       " '02c7744e-0f22-4b81-a507-7aa8d882c7c6',\n",
       " 'cc285cc3-ed8d-43a4-b8a0-3e0c4943ef7a',\n",
       " '9600e6e8-676e-44b3-a354-14577a028c8a',\n",
       " 'bdc6c295-1191-4b87-b0fa-330009c06f31',\n",
       " '21750e91-dd71-4a0f-973d-aa618e78615b',\n",
       " '642d52c3-d861-4019-bf9f-c105ab2eb0be',\n",
       " 'd0371e4a-1341-420f-9262-3c330c643dd5',\n",
       " '64a05096-a0f2-4ee7-88bc-139c7a9a8980',\n",
       " '3a5939ab-161b-48a3-ba49-27fc1a95bec9',\n",
       " '9f0e858d-a492-43b2-a922-1cee62def55e',\n",
       " 'ae047990-5317-422d-9632-c0dcd237eadd',\n",
       " 'a387b366-76ff-4858-9ca9-384491ac0ae8',\n",
       " 'b26efc58-2bcd-4f74-a21d-a65e69cac730',\n",
       " 'fff798ed-80ea-4016-a0b4-8e39efe38d37',\n",
       " '67502e7e-74bf-46d2-ab39-5ca6afb58414',\n",
       " '689b071a-9acb-4f24-8308-8c3e4cd2e1a4',\n",
       " 'a226a8d4-0c25-44e6-9677-523406626c9d',\n",
       " '8d4b42c3-8a38-4f1b-be97-13a8d650cb48',\n",
       " '6cdf64e2-de5b-4fdf-b7df-2be64e4e57d5',\n",
       " '5a622d91-4b9a-472a-a9a7-a9797e94afaa',\n",
       " 'b1fe4066-74ce-404c-94c9-87944a59ab6b',\n",
       " 'dee5d639-f8aa-4ddc-92f4-a57ae6b44f23',\n",
       " 'e18981c1-d31c-4e15-b6c2-33728dcef895',\n",
       " '328f2923-dedd-4b65-b48b-2f35cdbe0ce5',\n",
       " '2eb07c30-8aa0-4aa2-a923-743c055aeb74',\n",
       " '073e2e14-71e2-4899-a6fb-4b510819539f',\n",
       " '81ea5153-23c0-43af-8ca3-102191c943f8',\n",
       " '0bbdf4d5-4a22-4a4e-83b8-c477753e9c8e',\n",
       " 'a959947d-883a-4fd1-89fd-38e98ce024b6',\n",
       " '66ef6fcd-69f8-408a-846c-6cbff848af31',\n",
       " 'eaf9c5fd-9920-4849-9e5b-83d1b8fbfeef',\n",
       " '34f43d1a-8bdc-470e-8957-d22c5d9b9b5a',\n",
       " 'ee164cfe-8017-4975-b429-a8d1c602fa1c',\n",
       " '20c47143-e3a9-4fda-a7a1-57b706cb6186',\n",
       " '1b79a4b8-cdfc-4ca5-8217-41b935180b44',\n",
       " '12f73c51-dd13-4d8a-8a0e-8fc9f41950f3',\n",
       " 'f2b04eaa-0b66-48b3-b826-e424e62447ea',\n",
       " '081ffcb7-b521-48e9-b3e7-1b835f693133',\n",
       " 'aec5f299-2738-4438-96a5-0f22e78ea852',\n",
       " '5c80fd0b-71e7-4a33-82f9-2259394521a0',\n",
       " '1855405d-1499-4c66-a8ed-8328d4e3e21b',\n",
       " 'f47bc6dc-dca2-4be6-99cc-6cf6f952220c',\n",
       " 'c7f5f146-9fb2-4905-804f-76b6b9a599d2',\n",
       " 'f2287ba6-a5ac-4390-8972-286f6f1ebf5e',\n",
       " '26be9ef2-ac8c-4584-b59f-a80c0d727513',\n",
       " 'd668cdcc-4bd3-418b-8814-4128846cffdc',\n",
       " 'c2310df2-64c6-4b96-bc7a-b5204db36151',\n",
       " '60a92625-6a06-49bc-9a83-0a867f0d8197',\n",
       " 'cb4d03d1-79b9-46ac-b69c-3e5eb47636cc',\n",
       " '156c2b05-b0d1-44a9-a7b1-761f6f446e96',\n",
       " '89f691c4-f180-4a85-a4eb-f078b7ad15e7',\n",
       " 'c926c7e9-43ae-48eb-ac62-200b18734888',\n",
       " '9d6e5b90-3423-4bf3-8f60-e72b36d67f0b',\n",
       " '64176840-c387-40cc-a6de-5ff407d0d7f3',\n",
       " 'dd486c1b-48e5-4b9b-8186-469efd780939',\n",
       " '24d09891-3049-4f5e-b8f0-335451cde0bc',\n",
       " '2ec46ca1-3a4f-4091-8d34-0d1b7bc14f4b',\n",
       " '2481bd2f-8889-4d61-b569-9d8d2247cbf9']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90603d1-db9f-4ace-8cdd-2f9cfdc82b28",
   "metadata": {},
   "source": [
    "## Build Retrieval Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e11cbaef-3733-46fa-864b-d72a660f86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62535519-1b8c-459b-9db0-1d3baddcbc92",
   "metadata": {},
   "source": [
    "### 1. Generate a Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "652534ea-4163-4dd6-b2a8-9b70b443b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d79d1-795c-466c-bb80-e0e7df9d79d4",
   "metadata": {},
   "source": [
    "### 2. Query the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c9a1a47-2384-4fe7-b4ed-39f5aba7af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3704599-86af-41ce-b993-688e5f86c66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Safety\n",
      "WARNING: this section contains examples of text that may be considered unsafe, offensive, or upsetting.\n",
      "In this section, we dive deeper into the important topic of safety measurements and mitigations. We first\n",
      "discuss our safety investigations into pretraining data and pretrained models (Section 4.1). Next, we describe\n",
      "the process of our safety alignment (Section 4.2), explaining how we collected safety-related annotations and\n",
      "utilized SFT and RLHF, and present experimental results. Then, we discuss the red teaming we performed to\n",
      "further understand and improve model safety (Section 4.3). Finally, we present quantitative safety evaluations\n",
      "of Llama 2-Chat (Section 4.4). We also share a model card in the Appendix, in Table 52.\n",
      "4.1\n",
      "Safety in Pretraining\n",
      "It is important to understand what is in the pretraining data both to increase transparency and to shed\n",
      "light on root causes of potential downstream issues, such as potential biases. This can inform what, if any,\n",
      "downstream mitigations to consider, and help guide appropriate model use. In this section, we analyze the\n",
      "pretraining data for distributions of languages, demographic representations, and toxicity. We also present\n",
      "the results of testing the pretrained models on existing safety benchmarks.\n",
      "Steps Taken to Pretrain Responsibly.\n",
      "We followed Meta’s standard privacy and legal review processes for\n",
      "each dataset used in training. We did not use any Meta user data in training. We excluded data from certain\n",
      "sites known to contain a high volume of personal information about private individuals. We made a best\n",
      "effort to train our models efficiently to reduce the carbon footprint of pretraining (Section 2.2.1). Sharing our\n",
      "models broadly will reduce the need for others to train similar models. No additional filtering was conducted\n",
      "on the datasets, to allow Llama 2 to be more widely usable across tasks (e.g., it can be better used for hate\n",
      "speech classification), while avoiding the potential for the accidental demographic erasure sometimes caused\n",
      "by over-scrubbing. Importantly, this allows Llama 2-Chat to generalize more effectively during safety tuning\n",
      "with fewer examples (Welbl et al., 2021; Korbak et al., 2023; Xu et al., 2021). As a result, Llama 2 models\n",
      "should be used carefully and deployed only after significant safety tuning is applied.\n",
      "Demographic Representation: Pronouns.\n",
      "Bias in model generations may result from biases inherited\n",
      "from the training data itself. For instance, Bailey et al. (2022) shows that in massive text corpora, words\n",
      "representing “people” are often used in more similar contexts to words representing “men” than to words\n",
      "representing “women,” and Ganesh et al. (2023) demonstrates that a model’s performance on fairness metrics\n",
      "can be highly dependent on how the model trains on data representing underrepresented demographic\n",
      "groups. Within our English-language training corpus, we computed the frequencies of the most common\n",
      "English pronouns in Table 9a. We observe that He pronouns are generally overrepresented in documents\n",
      "compared to She pronouns, echoing similar frequency differences observed in pronominal usage for similarly\n",
      "sized model pretraining datasets (Chowdhery et al., 2022). This could mean that the model is learning less\n",
      "during pretraining about context that mentions She pronouns, and subsequently may potentially generate He\n",
      "pronouns at a higher rate than She pronouns.\n",
      "Demographic Representation: Identities.\n",
      "We also analyze the representation of different demographic\n",
      "groups in the pretraining data by measuring rates of usage of demographic identity terms from the HolisticBias\n",
      "dataset (Smith et al., 2022) as a proxy. We compute frequencies for each descriptor term in the pretraining\n",
      "corpus. We group descriptors into 5 axes (Religion, Gender and Sex, Nationality, Race and Ethnicity, and\n",
      "Sexual Orientation), and show the top 5 terms in each axis in Table 9b. In the top 5 terms, we remove a few\n",
      "terms such as “straight,” “white,” and “black,” because these terms have frequent uses beyond demographic\n",
      "mentions (e.g., as basic color terms). We also deduplicate across lists, removing a few terms found in\n",
      "both Gender and Sex and Sexual Orientation. For Gender and Sex, while She pronouns are mentioned\n",
      "in fewer documents, the term “female” is present in a larger percentage of documents. This could imply\n",
      "that while there is less frequent context about She pronouns, comments about “females” are more prevalent,\n",
      "perhaps reflecting the differences in linguistic markedness of these terms (Blodgett et al., 2021).\n"
     ]
    }
   ],
   "source": [
    "# returns a VectorStoreQueryResult\n",
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc95f59-baf9-43e9-9e1c-2364766ad27d",
   "metadata": {},
   "source": [
    "### 3. Parse Result into a Set of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29bc090b-dd58-404b-8e4a-fab4ffc73861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166202d8-bd61-4e66-9c5f-65745f076bc5",
   "metadata": {},
   "source": [
    "### 4. Put into a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ad47605-92f2-4a80-aeba-14e703a3ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6770ba03-cf3e-4e0e-b923-188d905e784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b700b9-4bee-4dd0-858c-c49c813129b2",
   "metadata": {},
   "source": [
    "## Plug this into our RetrieverQueryEngine to synthesize a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdfb6383-560c-4d64-95ec-dba4825d64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c23fd7dd-4415-42fa-8735-0c981c459a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 70B outperforms all open-source models.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "810c7f2f-81da-431f-980b-6652ebaac084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\n",
      "in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\n",
      "gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,\n",
      "2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4\n",
      "and PaLM-2-L.\n",
      "We also analysed the potential data contamination and share the details in Section A.6.\n",
      "Benchmark (shots)\n",
      "GPT-3.5\n",
      "GPT-4\n",
      "PaLM\n",
      "PaLM-2-L\n",
      "Llama 2\n",
      "MMLU (5-shot)\n",
      "70.0\n",
      "86.4\n",
      "69.3\n",
      "78.3\n",
      "68.9\n",
      "TriviaQA (1-shot)\n",
      "–\n",
      "–\n",
      "81.4\n",
      "86.1\n",
      "85.0\n",
      "Natural Questions (1-shot)\n",
      "–\n",
      "–\n",
      "29.3\n",
      "37.5\n",
      "33.0\n",
      "GSM8K (8-shot)\n",
      "57.1\n",
      "92.0\n",
      "56.5\n",
      "80.7\n",
      "56.8\n",
      "HumanEval (0-shot)\n",
      "48.1\n",
      "67.0\n",
      "26.2\n",
      "–\n",
      "29.9\n",
      "BIG-Bench Hard (3-shot)\n",
      "–\n",
      "–\n",
      "52.3\n",
      "65.7\n",
      "51.2\n",
      "Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4\n",
      "are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the\n",
      "PaLM-2-L are from Anil et al. (2023).\n",
      "3\n",
      "Fine-tuning\n",
      "Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques,\n",
      "including both instruction tuning and RLHF, requiring significant computational and annotation resources.\n",
      "In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as\n",
      "well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a\n",
      "new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns\n",
      "(Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
